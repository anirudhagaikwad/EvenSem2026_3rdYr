# Mining frequent itemsets without candidate generation
primarily uses the FP-Growth (Frequent Pattern Growth) algorithm, which avoids the costly candidate set creation of methods like Apriori by compressing the database into a compact FP-Tree (Frequent Pattern Tree), allowing direct mining of patterns with just two database scans, making it efficient for large datasets. This divide-and-conquer method builds the tree, then recursively mines conditional pattern bases and smaller FP-Trees to find patterns, saving significant computation.

The FP Growth algorithm in data mining is a popular method for frequent pattern mining. The algorithm is efficient for mining frequent item sets in large datasets. It works by constructing a frequent pattern tree (FP-tree) from the input dataset. FP Growth algorithm was developed by Han in 2000 and is a powerful tool for frequent pattern mining in data mining. It is widely used in various applications such as market basket analysis, bioinformatics, and web usage mining.

# FP Growth in Data Mining
The FP Growth algorithm is a popular method for frequent pattern mining in data mining. It works by constructing a frequent pattern tree (FP-tree) from the input dataset. The FP-tree is a compressed representation of the dataset that captures the frequency and association information of the items in the data.

The algorithm first scans the dataset and maps each transaction to a path in the tree. Items are ordered in each transaction based on their frequency, with the most frequent items appearing first. Once the FP tree is constructed, frequent itemsets can be generated by recursively mining the tree. This is done by starting at the bottom of the tree and working upwards, finding all combinations of itemsets that satisfy the minimum support threshold.

The FP Growth algorithm in data mining has several advantages over other frequent pattern mining algorithms, such as Apriori. The Apriori algorithm is not suitable for handling large datasets because it generates a large number of candidates and requires multiple scans of the database to my frequent items. In comparison, the FP Growth algorithm requires only a single scan of the data and a small amount of memory to construct the FP tree. It can also be parallelized to improve performance.

# Working on FP Growth Algorithm
The working of the FP Growth algorithm in data mining can be summarized in the following steps:
➜ Scan the database:
    In this step, the algorithm scans the input dataset to determine the frequency of each item. This determines the order in which items are added to the FP tree, with the most frequent items added first.
➜ Sort items:
    In this step, the items in the dataset are sorted in descending order of frequency. The infrequent items that do not meet the minimum support threshold are removed from the dataset. This helps to reduce the dataset's size and improve the algorithm's efficiency.
➜ Construct the FP-tree:
    In this step, the FP-tree is constructed. The FP-tree is a compact data structure that stores the frequent itemsets and their support counts.
    Generate frequent itemsets:
    Once the FP-tree has been constructed, frequent itemsets can be generated by recursively mining the tree. Starting at the bottom of the tree, the algorithm finds all combinations of frequent item sets that satisfy the minimum support threshold.
➜Generate association rules:
    Once all frequent item sets have been generated, the algorithm post-processes the generated frequent item sets to generate association rules, which can be used to identify interesting relationships between the items in the dataset.

# FP Tree
The FP-tree (Frequent Pattern tree) is a data structure used in the FP Growth algorithm for frequent pattern mining. It represents the frequent itemsets in the input dataset compactly and efficiently. The FP tree consists of the following components:
➜ Root Node:
    The root node of the FP-tree represents an empty set. It has no associated item but a pointer to the first node of each item in the tree.
➜Item Node:
    Each item node in the FP-tree represents a unique item in the dataset. It stores the item name and the frequency count of the item in the dataset.
➜Header Table:
    The header table lists all the unique items in the dataset, along with their frequency count. It is used to track each item's location in the FP tree.
➜Child Node:
    Each child node of an item node represents an item that co-occurs with the item the parent node represents in at least one transaction in the dataset.
➜Node Link:
    The node-link is a pointer that connects each item in the header table to the first node of that item in the FP-tree. It is used to traverse the conditional pattern base of each item during the mining process.

The FP tree is constructed by scanning the input dataset and inserting each transaction into the tree one at a time. For each transaction, the items are sorted in descending order of frequency count and then added to the tree in that order. If an item exists in the tree, its frequency count is incremented, and a new path is created from the existing node. If an item does not exist in the tree, a new node is created for that item, and a new path is added to the tree. We will understand in detail how FP-tree is constructed in the #5_FP-Growth_algo_Example.md section.

# FP-Growth Algorithm vs. Apriori Algorithm

| Factor              | FP-Growth Algorithm                                                                 | Apriori Algorithm                                                                 |
|---------------------|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **Working**         | Uses FP-tree (Frequent Pattern tree) to mine frequent itemsets without candidate generation | Mines frequent itemsets iteratively (1-itemsets → 2-itemsets → 3-itemsets, etc.) using candidate generation |
| **Candidate Generation** | No explicit candidate generation; constructs FP-Tree and mines using conditional pattern bases | Generates candidate itemsets using join and prune steps (Apriori property)        |
| **Data Scanning**   | Scans the database **only twice** (once for frequency count, once for FP-Tree construction) | Scans the database **multiple times** (once for each level of itemset size)       |
| **Memory Usage**    | Lower memory requirement – FP-Tree is a compressed representation of the database   | Higher memory requirement – needs to store all candidate itemsets in memory       |
| **Speed**           | Generally faster, especially for dense datasets, due to compressed structure and no candidate generation | Slower, particularly when there are many candidates or large number of transactions |
| **Scalability**     | Scales better with large datasets due to compact FP-Tree structure                  | Performs poorly on very large datasets due to exponential candidate growth         |
| **Best Suited For** | Dense datasets, large number of transactions, when memory is a concern               | Sparse datasets, smaller number of items, when simplicity is preferred             |
